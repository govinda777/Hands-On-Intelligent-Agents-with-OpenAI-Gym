{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Deep Q-learning agent implemented using PyTorch | Praveen Palanisamy\n",
    "# Chapter 6, Hands-on Intelligent Agents with OpenAI Gym, 2018\n",
    "\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import environment.atari as Atari\n",
    "import environment.utils as env_utils\n",
    "from utils.params_manager import ParamsManager\n",
    "from utils.decay_schedule import LinearDecaySchedule\n",
    "from utils.experience_memory import Experience, ExperienceMemory\n",
    "import utils.weights_initializer\n",
    "from function_approximator.perceptron import SLP\n",
    "from function_approximator.cnn import CNN\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "args = ArgumentParser(\"deep_Q_learner\")\n",
    "args.add_argument(\"--params-file\", help=\"Path to the parameters json file. Default is parameters.json\",\n",
    "                  default=\"parameters.json\", metavar=\"PFILE\")\n",
    "args.add_argument(\"--env\", help=\"ID of the Atari environment available in OpenAI Gym.Default is SeaquestNoFrameskip-v4\",\n",
    "                  default=\"SeaquestNoFrameskip-v4\", metavar=\"ENV\")\n",
    "args.add_argument(\"--gpu-id\", help=\"GPU device ID to use. Default=0\", default=0, type=int, metavar=\"GPU_ID\")\n",
    "args.add_argument(\"--render\", help=\"Render environment to Screen. Off by default\", action=\"store_true\", default=False)\n",
    "args.add_argument(\"--test\", help=\"Test mode. Used for playing without learning. Off by default\", action=\"store_true\",\n",
    "                  default=False)\n",
    "args.add_argument(\"--record\", help=\"Enable recording (video & stats) of the agent's performance\",\n",
    "                  action=\"store_true\", default=False)\n",
    "args.add_argument(\"--recording-output-dir\", help=\"Directory to store monitor outputs. Default=./trained_models/results\",\n",
    "                  default=\"./trained_models/results\")\n",
    "# In a notebook, we can't use argparse like this. We'll simulate it or set params directly.\n",
    "# For now, I'll comment out the parsing and set default values.\n",
    "# args = args.parse_args()\n",
    "\n",
    "# Let's create a class to hold the arguments, simulating argparse's behavior\n",
    "class Args:\n",
    "    params_file = 'parameters.json'\n",
    "    env = 'SeaquestNoFrameskip-v4'\n",
    "    gpu_id = 0\n",
    "    render = False\n",
    "    test = False\n",
    "    record = False\n",
    "    recording_output_dir = './trained_models/results'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "params_manager= ParamsManager(args.params_file)\n",
    "seed = params_manager.get_agent_params()['seed']\n",
    "summary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']\n",
    "summary_file_path= summary_file_path_prefix + args.env+ \"_\" + datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "writer = SummaryWriter(summary_file_path)\n",
    "# Export the parameters as json files to the log directory to keep track of the parameters used in each experiment\n",
    "params_manager.export_env_params(summary_file_path + \"/\" + \"env_params.json\")\n",
    "params_manager.export_agent_params(summary_file_path + \"/\" + \"agent_params.json\")\n",
    "global_step_num = 0\n",
    "use_cuda = params_manager.get_agent_params()['use_cuda']\n",
    "# new in PyTorch 0.4\n",
    "device = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "class Deep_Q_Learner(object):\n",
    "    def __init__(self, state_shape, action_shape, params):\n",
    "        \"\"\"\n",
    "        self.Q is the Action-Value function. This agent represents Q using a Neural Network\n",
    "        If the input is a single dimensional vector, uses a Single-Layer-Perceptron else if the input is 3 dimensional\n",
    "        image, use a Convolutional-Neural-Network\n",
    "\n",
    "        :param state_shape: Shape (tuple) of the observation/state\n",
    "        :param action_shape: Shape (number) of the discrete action space\n",
    "        :param params: A dictionary containing various Agent configuration parameters and hyper-parameters\n",
    "        \"\"\"\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.params = params\n",
    "        self.gamma = self.params['gamma']  # Agent's discount factor\n",
    "        self.learning_rate = self.params['lr']  # Agent's Q-learning rate\n",
    "        self.best_mean_reward = - float(\"inf\") # Agent's personal best mean episode reward\n",
    "        self.best_reward = - float(\"inf\")\n",
    "        self.training_steps_completed = 0  # Number of training batch steps completed so far\n",
    "\n",
    "        if len(self.state_shape) == 1:  # Single dimensional observation/state space\n",
    "            self.DQN = SLP\n",
    "        elif len(self.state_shape) == 3:  # 3D/image observation/state\n",
    "            self.DQN = CNN\n",
    "\n",
    "        self.Q = self.DQN(state_shape, action_shape, device).to(device)\n",
    "        self.Q.apply(utils.weights_initializer.xavier)\n",
    "\n",
    "        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.learning_rate)\n",
    "        if self.params['use_target_network']:\n",
    "            self.Q_target = self.DQN(state_shape, action_shape, device).to(device)\n",
    "        # self.policy is the policy followed by the agent. This agents follows\n",
    "        # an epsilon-greedy policy w.r.t it's Q estimate.\n",
    "        self.policy = self.epsilon_greedy_Q\n",
    "        self.epsilon_max = params[\"epsilon_max\"]\n",
    "        self.epsilon_min = params[\"epsilon_min\"]\n",
    "        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,\n",
    "                                    final_value=self.epsilon_min,\n",
    "                                    max_steps= self.params['epsilon_decay_final_step'])\n",
    "        self.step_num = 0\n",
    "\n",
    "        self.memory = ExperienceMemory(capacity=int(self.params['experience_memory_capacity']))  # Initialize an Experience memory with 1M capacity\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        observation = np.array(observation)  # Observations could be lazy frames. So force fetch before moving forward\n",
    "        observation = observation / 255.0  # Scale/Divide by max limit of obs' dtype. 255 for uint8\n",
    "        if len(observation.shape) == 3: # Single image (not a batch)\n",
    "            if observation.shape[2] < observation.shape[0]:  # Probably observation is in W x H x C format\n",
    "                # NOTE: This is just an additional check. The env wrappers are taking care of this conversion already\n",
    "                # Reshape to C x H x W format as per PyTorch's convention\n",
    "                observation = observation.reshape(observation.shape[2], observation.shape[1], observation.shape[0])\n",
    "            observation = np.expand_dims(observation, 0)  # Create a batch dimension\n",
    "        return self.policy(observation)\n",
    "\n",
    "    def epsilon_greedy_Q(self, observation):\n",
    "        # Decay Epsilon/exploration as per schedule\n",
    "        writer.add_scalar(\"DQL/epsilon\", self.epsilon_decay(self.step_num), self.step_num)\n",
    "        self.step_num +=1\n",
    "        if random.random() < self.epsilon_decay(self.step_num) and not self.params[\"test\"]:\n",
    "            action = random.choice([i for i in range(self.action_shape)])\n",
    "        else:\n",
    "            action = np.argmax(self.Q(observation).data.to(torch.device('cpu')).numpy())\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_next, done):\n",
    "        # TD(0) Q-learning\n",
    "        if done:  # End of episode\n",
    "            td_target = reward + 0.0  # Set the value of terminal state to zero\n",
    "        else:\n",
    "            td_target = r + self.gamma * torch.max(self.Q(s_next))\n",
    "        td_error = td_target - self.Q(s)[a]\n",
    "        # Update Q estimate\n",
    "        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error\n",
    "        self.Q_optimizer.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.Q_optimizer.step()\n",
    "\n",
    "    def learn_from_batch_experience(self, experiences):\n",
    "        batch_xp = Experience(*zip(*experiences))\n",
    "        obs_batch = np.array(batch_xp.obs) / 255.0  # Scale/Divide by max limit of obs's dtype. 255 for uint8\n",
    "        action_batch = np.array(batch_xp.action)\n",
    "        reward_batch = np.array(batch_xp.reward)\n",
    "        # Clip the rewards\n",
    "        if self.params[\"clip_rewards\"]:\n",
    "            reward_batch = np.sign(reward_batch)\n",
    "        next_obs_batch = np.array(batch_xp.next_obs) / 255.0  # Scale/Divide by max limit of obs' dtype. 255 for uint8\n",
    "        done_batch = np.array(batch_xp.done)\n",
    "\n",
    "        if self.params['use_target_network']:\n",
    "            #if self.training_steps_completed % self.params['target_network_update_freq'] == 0:\n",
    "            if self.step_num % self.params['target_network_update_freq'] == 0:\n",
    "                # The *update_freq is the Num steps after which target net is updated.\n",
    "                # A schedule can be used instead to vary the update freq.\n",
    "                self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "            td_target = reward_batch + ~done_batch * \\\n",
    "                np.tile(self.gamma, len(next_obs_batch)) * \\\n",
    "                self.Q_target(next_obs_batch).max(1)[0].data.cpu().numpy()\n",
    "        else:\n",
    "            td_target = reward_batch + ~done_batch * \\\n",
    "                np.tile(self.gamma, len(next_obs_batch)) * \\\n",
    "                self.Q(next_obs_batch).detach().max(1)[0].data.cpu().numpy()\n",
    "\n",
    "        td_target = torch.from_numpy(td_target).to(device)\n",
    "        action_idx = torch.from_numpy(action_batch).to(device)\n",
    "        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),\n",
    "                                                       td_target.float().unsqueeze(1))\n",
    "\n",
    "        self.Q_optimizer.zero_grad()\n",
    "        td_error.mean().backward()\n",
    "        writer.add_scalar(\"DQL/td_error\", td_error.mean(), self.step_num)\n",
    "        self.Q_optimizer.step()\n",
    "\n",
    "    def replay_experience(self, batch_size = None):\n",
    "        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']\n",
    "        experience_batch = self.memory.sample(batch_size)\n",
    "        self.learn_from_batch_experience(experience_batch)\n",
    "        self.training_steps_completed += 1  # Increment the number of training batch steps complemented\n",
    "\n",
    "    def save(self, env_name):\n",
    "        file_name = self.params['save_dir'] + \"DQL_\" + env_name + \".ptm\"\n",
    "        agent_state = {\"Q\": self.Q.state_dict(),\n",
    "                       \"best_mean_reward\": self.best_mean_reward,\n",
    "                       \"best_reward\": self.best_reward};\n",
    "        torch.save(agent_state, file_name)\n",
    "        print(\"Agent's state saved to \", file_name)\n",
    "\n",
    "    def load(self, env_name):\n",
    "        file_name = self.params['load_dir'] + \"DQL_\" + env_name + \".ptm\"\n",
    "        agent_state = torch.load(file_name, map_location= lambda storage, loc: storage)\n",
    "        self.Q.load_state_dict(agent_state[\"Q\"])\n",
    "        self.Q.to(device)\n",
    "        self.best_mean_reward = agent_state[\"best_mean_reward\"]\n",
    "        self.best_reward = agent_state[\"best_reward\"]\n",
    "        print(\"Loaded Q model state from\", file_name,\n",
    "              \" which fetched a best mean reward of:\", self.best_mean_reward,\n",
    "              \" and an all time best reward of:\", self.best_reward)\n",
    "\n",
    "# This block will be the main execution part of the notebook\n",
    "if __name__ == \"__main__\":\n",
    "    env_conf = params_manager.get_env_params()\n",
    "    env_conf[\"env_name\"] = args.env\n",
    "    # In test mode, let the end of the game be the end of episode rather than ending episode at the end of every life.\n",
    "    # This helps to report out the (mean and max) episode rewards per game (rather than per life!)\n",
    "    if args.test:\n",
    "        env_conf[\"episodic_life\"] = False\n",
    "    # Specify the reward calculation type used for printing stats at the end of every episode.\n",
    "    # If \"episode_life\" is true, the printed stats (reward, mean reward, max reward) are per life. If \"episodic_life\"\n",
    "    # is false, the printed stats/scores are per game in Atari environments\n",
    "    rew_type = \"LIFE\" if env_conf[\"episodic_life\"] else \"GAME\"\n",
    "\n",
    "    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default\n",
    "    custom_region_available = False\n",
    "    for key, value in env_conf['useful_region'].items():\n",
    "        if key in args.env:\n",
    "            env_conf['useful_region'] = value\n",
    "            custom_region_available = True\n",
    "            break\n",
    "    if custom_region_available is not True:\n",
    "        env_conf['useful_region'] = env_conf['useful_region']['Default']\n",
    "\n",
    "    print(\"Using env_conf:\", env_conf)\n",
    "    atari_env = False\n",
    "    for game in Atari.get_games_list():\n",
    "        if game.replace(\"_\", \"\") in args.env.lower():\n",
    "            atari_env = True\n",
    "    if atari_env:\n",
    "        env = Atari.make_env(args.env, env_conf)\n",
    "    else:\n",
    "        print(\"Given environment name is not an Atari Env. Creating a Gym env\")\n",
    "        # Resize the obs to w x h (84 x 84 by default) and then reshape it to be in the C x H x W format\n",
    "        env = env_utils.ResizeReshapeFrames(gym.make(args.env))\n",
    "\n",
    "    if args.record:  # If monitor is enabled, record stats and video of agent's performance\n",
    "        env = gym.wrappers.Monitor(env, args.recording_output_dir, force=True)\n",
    "\n",
    "    observation_shape = env.observation_space.shape\n",
    "    action_shape = env.action_space.n\n",
    "    agent_params = params_manager.get_agent_params()\n",
    "    agent_params[\"test\"] = args.test\n",
    "    agent = Deep_Q_Learner(observation_shape, action_shape, agent_params)\n",
    "\n",
    "    episode_rewards = list()\n",
    "    prev_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
    "    num_improved_episodes_before_checkpoint = 0  # To keep track of the num of ep with higher perf to save model\n",
    "    print(\"Using agent_params:\", agent_params)\n",
    "    if agent_params['load_trained_model']:\n",
    "        try:\n",
    "            agent.load(env_conf[\"env_name\"])\n",
    "            prev_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
    "        except FileNotFoundError:\n",
    "            print(\"WARNING: No trained model found for this environment. Training from scratch.\")\n",
    "\n",
    "    #for episode in range(agent_params['max_num_episodes']):\n",
    "    episode = 0\n",
    "    while global_step_num <= agent_params['max_training_steps']:\n",
    "        obs = env.reset()\n",
    "        cum_reward = 0.0  # Cumulative reward\n",
    "        done = False\n",
    "        step = 0\n",
    "        #for step in range(agent_params['max_steps_per_episode']):\n",
    "        while not done:\n",
    "            if env_conf['render'] or args.render:\n",
    "                env.render()\n",
    "            action = agent.get_action(obs)\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            #agent.learn(obs, action, reward, next_obs, done)\n",
    "            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n",
    "\n",
    "            obs = next_obs\n",
    "            cum_reward += reward\n",
    "            step += 1\n",
    "            global_step_num +=1\n",
    "\n",
    "            if done is True:\n",
    "                episode += 1\n",
    "                episode_rewards.append(cum_reward)\n",
    "                if cum_reward > agent.best_reward:\n",
    "                    agent.best_reward = cum_reward\n",
    "                if np.mean(episode_rewards) > prev_checkpoint_mean_ep_rew:\n",
    "                    num_improved_episodes_before_checkpoint += 1\n",
    "                if num_improved_episodes_before_checkpoint >= agent_params[\"save_freq_when_perf_improves\"]:\n",
    "                    prev_checkpoint_mean_ep_rew = np.mean(episode_rewards)\n",
    "                    agent.best_mean_reward = np.mean(episode_rewards)\n",
    "                    agent.save(env_conf['env_name'])\n",
    "                    num_improved_episodes_before_checkpoint = 0\n",
    "                print(\"\\nEpisode#{} ended in {} steps. Per {} stats: reward ={} ; mean_reward={:.3f} best_reward={}\".\n",
    "                      format(episode, step+1, rew_type, cum_reward, np.mean(episode_rewards), agent.best_reward))\n",
    "                writer.add_scalar(\"main/ep_reward\", cum_reward, global_step_num)\n",
    "                writer.add_scalar(\"main/mean_ep_reward\", np.mean(episode_rewards), global_step_num)\n",
    "                writer.add_scalar(\"main/max_ep_rew\", agent.best_reward, global_step_num)\n",
    "                # Learn from batches of experience once a certain amount of xp is available unless in test only mode\n",
    "                if agent.memory.get_size() >= 2 * agent_params['replay_start_size'] and not args.test:\n",
    "                    agent.replay_experience()\n",
    "\n",
    "                break\n",
    "    env.close()\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
