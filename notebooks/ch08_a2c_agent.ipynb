{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# n-step Advantage Actor-Critic Agent (A2C) | Praveen Palanisamy\n",
    "# Chapter 8, Hands-on Intelligent Agents with OpenAI Gym, 2018\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "try:\n",
    "    import roboschool\n",
    "except ImportError:\n",
    "    pass\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils.params_manager import ParamsManager\n",
    "from function_approximator.shallow import Actor as ShallowActor\n",
    "from function_approximator.shallow import DiscreteActor as ShallowDiscreteActor\n",
    "from function_approximator.shallow import Critic as ShallowCritic\n",
    "from function_approximator.deep import Actor as DeepActor\n",
    "from function_approximator.deep import DiscreteActor as DeepDiscreteActor\n",
    "from function_approximator.deep import Critic as DeepCritic\n",
    "import environment.carla_gym\n",
    "import environment.atari as Atari\n",
    "\n",
    "# In a notebook, we can't use argparse. We'll define the arguments in a class.\n",
    "class Args:\n",
    "    env = 'Pendulum-v0'\n",
    "    params_file = 'a2c_parameters.json'\n",
    "    model_dir = 'trained_models/'\n",
    "    render = False\n",
    "    test = False\n",
    "    gpu_id = 0\n",
    "\n",
    "args = Args()\n",
    "\n",
    "global_step_num = 0\n",
    "params_manager= ParamsManager(args.params_file)\n",
    "summary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']\n",
    "summary_file_path= summary_file_path_prefix + args.env + \"_\" + datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "writer = SummaryWriter(summary_file_path)\n",
    "# Export the parameters as json files to the log directory to keep track of the parameters used in each experiment\n",
    "params_manager.export_env_params(summary_file_path + \"/\" + \"env_params.json\")\n",
    "params_manager.export_agent_params(summary_file_path + \"/\" + \"agent_params.json\")\n",
    "use_cuda = params_manager.get_agent_params()['use_cuda']\n",
    "# Introduced in PyTorch 0.4\n",
    "device = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "seed = params_manager.get_agent_params()['seed']  # With the intent to make the results reproducible\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "Transition = namedtuple(\"Transition\", [\"s\", \"value_s\", \"a\", \"log_prob_a\"])\n",
    "\n",
    "class DeepActorCriticAgent(mp.Process):\n",
    "    def __init__(self, id, env_name, agent_params, env_params):\n",
    "        \"\"\"\n",
    "        An Advantage Actor-Critic Agent that uses a Deep Neural Network to represent it's Policy and the Value function\n",
    "        :param id: An integer ID to identify the agent in case there are multiple agent instances\n",
    "        :param env_name: Name/ID of the environment\n",
    "        :param agent_params: Parameters to be used by the agent\n",
    "        \"\"\"\n",
    "        super(DeepActorCriticAgent, self).__init__()\n",
    "        self.id = id\n",
    "        self.actor_name = \"actor\" + str(self.id)\n",
    "        self.env_name = env_name\n",
    "        self.params = agent_params\n",
    "        self.env_conf = env_params\n",
    "        self.policy = self.multi_variate_gaussian_policy\n",
    "        self.gamma = self.params['gamma']\n",
    "        self.trajectory = []  # Contains the trajectory of the agent as a sequence of Transitions\n",
    "        self.rewards = []  #  Contains the rewards obtained from the env at every step\n",
    "        self.global_step_num = 0\n",
    "        self.best_mean_reward = - float(\"inf\") # Agent's personal best mean episode reward\n",
    "        self.best_reward = - float(\"inf\")\n",
    "        self.saved_params = False  # Whether or not the params have been saved along with the model to model_dir\n",
    "        self.continuous_action_space = True  #Assumption by default unless env.action_space is Discrete\n",
    "\n",
    "    def multi_variate_gaussian_policy(self, obs):\n",
    "        \"\"\"\n",
    "        Calculates a multi-variate gaussian distribution over actions given observations\n",
    "        :param obs: Agent's observation\n",
    "        :return: policy, a distribution over actions for the given observation\n",
    "        \"\"\"\n",
    "        mu, sigma = self.actor(obs)\n",
    "        value = self.critic(obs)\n",
    "        [ mu[:, i].clamp_(float(self.env.action_space.low[i]), float(self.env.action_space.high[i]))\n",
    "         for i in range(self.action_shape)]  # Clamp each dim of mu based on the (low,high) limits of that action dim\n",
    "        sigma = torch.nn.Softplus()(sigma).squeeze() + 1e-7  # Let sigma be (smoothly) +ve\n",
    "        self.mu = mu.to(torch.device(\"cpu\"))\n",
    "        self.sigma = sigma.to(torch.device(\"cpu\"))\n",
    "        self.value = value.to(torch.device(\"cpu\"))\n",
    "        if len(self.mu.shape) == 0: # See if mu is a scalar\n",
    "            #self.mu = self.mu.unsqueeze(0)  # This prevents MultivariateNormal from crashing with SIGFPE\n",
    "            self.mu.unsqueeze_(0)\n",
    "        self.action_distribution = MultivariateNormal(self.mu, torch.eye(self.action_shape) * self.sigma, validate_args=True)\n",
    "        return self.action_distribution\n",
    "\n",
    "    def discrete_policy(self, obs):\n",
    "        \"\"\"\n",
    "        Calculates a discrete/categorical distribution over actions given observations\n",
    "        :param obs: Agent's observation\n",
    "        :return: policy, a distribution over actions for the given observation\n",
    "        \"\"\"\n",
    "        logits = self.actor(obs)\n",
    "        value = self.critic(obs)\n",
    "        self.logits = logits.to(torch.device(\"cpu\"))\n",
    "        self.value = value.to(torch.device(\"cpu\"))\n",
    "        self.action_distribution = Categorical(logits=self.logits)\n",
    "        return self.action_distribution\n",
    "\n",
    "    def preproc_obs(self, obs):\n",
    "        obs = np.array(obs)  # Obs could be lazy frames. So, force fetch before moving forward\n",
    "        if len(obs.shape) == 3:\n",
    "            #  Reshape obs from (H x W x C) order to this order: C x W x H and resize to (C x 84 x 84)\n",
    "            obs = np.reshape(obs, (obs.shape[2], obs.shape[1], obs.shape[0]))\n",
    "            obs = np.resize(obs, (obs.shape[0], 84, 84))\n",
    "        #  Convert to torch Tensor, add a batch dimension, convert to float repr\n",
    "        obs = torch.from_numpy(obs).unsqueeze(0).float()\n",
    "        return obs\n",
    "\n",
    "    def process_action(self, action):\n",
    "        if self.continuous_action_space:\n",
    "            [action[:, i].clamp_(float(self.env.action_space.low[i]), float(self.env.action_space.high[i]))\n",
    "             for i in range(self.action_shape)]  # Limit the action to lie between the (low, high) limits of the env\n",
    "        action = action.to(torch.device(\"cpu\"))\n",
    "        return action.numpy().squeeze(0)  # Convert to numpy ndarray, squeeze and remove the batch dimension\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        obs = self.preproc_obs(obs)\n",
    "        action_distribution = self.policy(obs)  # Call to self.policy(obs) also populates self.value with V(obs)\n",
    "        value = self.value\n",
    "        action = action_distribution.sample()\n",
    "        log_prob_a = action_distribution.log_prob(action)\n",
    "        action = self.process_action(action)\n",
    "        # Store the n-step trajectory while training. Skip storing the trajectories in test mode\n",
    "        if not self.params[\"test\"]:\n",
    "            self.trajectory.append(Transition(obs, value, action, log_prob_a))  # Construct the trajectory\n",
    "        return action\n",
    "\n",
    "    def calculate_n_step_return(self, n_step_rewards, final_state, done, gamma):\n",
    "        \"\"\"\n",
    "        Calculates the n-step return for each state in the input-trajectory/n_step_transitions\n",
    "        :param n_step_rewards: List of rewards for each step\n",
    "        :param final_state: Final state in this n_step_transition/trajectory\n",
    "        :param done: True rf the final state is a terminal state if not, False\n",
    "        :return: The n-step return for each state in the n_step_transitions\n",
    "        \"\"\"\n",
    "        g_t_n_s = list()\n",
    "        with torch.no_grad():\n",
    "            g_t_n = torch.tensor([[0]]).float() if done else self.critic(self.preproc_obs(final_state)).cpu()\n",
    "            for r_t in n_step_rewards[::-1]:  # Reverse order; From r_tpn to r_t\n",
    "                g_t_n = torch.tensor(r_t).float() + self.gamma * g_t_n\n",
    "                g_t_n_s.insert(0, g_t_n)  # n-step returns inserted to the left to maintain correct index order\n",
    "            return g_t_n_s\n",
    "\n",
    "    def calculate_loss(self, trajectory, td_targets):\n",
    "        \"\"\"\n",
    "        Calculates the critic and actor losses using the td_targets and self.trajectory\n",
    "        :param td_targets:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n_step_trajectory = Transition(*zip(*trajectory))\n",
    "        v_s_batch = n_step_trajectory.value_s\n",
    "        log_prob_a_batch = n_step_trajectory.log_prob_a\n",
    "        actor_losses, critic_losses = [], []\n",
    "        for td_target, critic_prediction, log_p_a in zip(td_targets, v_s_batch, log_prob_a_batch):\n",
    "            td_err = td_target - critic_prediction\n",
    "            actor_losses.append(- log_p_a * td_err)  # td_err is an unbiased estimated of Advantage\n",
    "            critic_losses.append(F.smooth_l1_loss(critic_prediction, td_target))\n",
    "            #critic_loss.append(F.mse_loss(critic_pred, td_target))\n",
    "        if self.params[\"use_entropy_bonus\"]:\n",
    "            actor_loss = torch.stack(actor_losses).mean() - self.action_distribution.entropy().mean()\n",
    "        else:\n",
    "            actor_loss = torch.stack(actor_losses).mean()\n",
    "        critic_loss = torch.stack(critic_losses).mean()\n",
    "\n",
    "        writer.add_scalar(self.actor_name + \"/critic_loss\", critic_loss, self.global_step_num)\n",
    "        writer.add_scalar(self.actor_name + \"/actor_loss\", actor_loss, self.global_step_num)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def learn(self, n_th_observation, done):\n",
    "        if self.params[\"clip_rewards\"]:\n",
    "            self.rewards = np.sign(self.rewards).tolist()  # Clip rewards to -1 or 0 or +1\n",
    "        td_targets = self.calculate_n_step_return(self.rewards, n_th_observation, done, self.gamma)\n",
    "        actor_loss, critic_loss = self.calculate_loss(self.trajectory, td_targets)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.trajectory.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def save(self):\n",
    "        model_file_name = self.params[\"model_dir\"] + \"A2C_\" + self.env_name + \".ptm\"\n",
    "        agent_state = {\"Actor\": self.actor.state_dict(),\n",
    "                       \"Critic\": self.critic.state_dict(),\n",
    "                       \"best_mean_reward\": self.best_mean_reward,\n",
    "                       \"best_reward\": self.best_reward}\n",
    "        torch.save(agent_state, model_file_name)\n",
    "        print(\"Agent's state is saved to\", model_file_name)\n",
    "        # Export the params used if not exported already\n",
    "        if not self.saved_params:\n",
    "            params_manager.export_agent_params(model_file_name + \".agent_params\")\n",
    "            print(\"The parameters have been saved to\", model_file_name + \".agent_params\")\n",
    "            self.saved_params = True\n",
    "\n",
    "    def load(self):\n",
    "        model_file_name = self.params[\"model_dir\"] + \"A2C_\" + self.env_name + \".ptm\"\n",
    "        agent_state = torch.load(model_file_name, map_location= lambda storage, loc: storage)\n",
    "        self.actor.load_state_dict(agent_state[\"Actor\"])\n",
    "        self.critic.load_state_dict(agent_state[\"Critic\"])\n",
    "        self.actor.to(device)\n",
    "        self.critic.to(device)\n",
    "        self.best_mean_reward = agent_state[\"best_mean_reward\"]\n",
    "        self.best_reward = agent_state[\"best_reward\"]\n",
    "        print(\"Loaded Advantage Actor-Critic model state from\", model_file_name,\n",
    "              \" which fetched a best mean reward of:\", self.best_mean_reward,\n",
    "              \" and an all time best reward of:\", self.best_reward)\n",
    "\n",
    "    def run(self):\n",
    "        # If a custom useful_region configuration for this environment ID is available, use it if not use the Default.\n",
    "        # Currently this is utilized for only the Atari env. Follows the same procedure as in Chapter 6\n",
    "        custom_region_available = False\n",
    "        for key, value in self.env_conf['useful_region'].items():\n",
    "            if key in args.env:\n",
    "                self.env_conf['useful_region'] = value\n",
    "                custom_region_available = True\n",
    "                break\n",
    "        if custom_region_available is not True:\n",
    "            self.env_conf['useful_region'] = self.env_conf['useful_region']['Default']\n",
    "        atari_env = False\n",
    "        for game in Atari.get_games_list():\n",
    "            if game.replace(\"_\", \"\") in args.env.lower():\n",
    "                atari_env = True\n",
    "        if atari_env:  # Use the Atari wrappers (like we did in Chapter 6) if it's an Atari env\n",
    "            self.env = Atari.make_env(self.env_name, self.env_conf)\n",
    "        else:\n",
    "            #print(\"Given environment name is not an Atari Env. Creating a Gym env\")\n",
    "            self.env = gym.make(self.env_name)\n",
    "\n",
    "        self.state_shape = self.env.observation_space.shape\n",
    "        if isinstance(self.env.action_space.sample(), int):  # Discrete action space\n",
    "            self.action_shape = self.env.action_space.n\n",
    "            self.policy = self.discrete_policy\n",
    "            self.continuous_action_space = False\n",
    "\n",
    "        else:  # Continuous action space\n",
    "            self.action_shape = self.env.action_space.shape[0]\n",
    "            self.policy = self.multi_variate_gaussian_policy\n",
    "        self.critic_shape = 1\n",
    "        if len(self.state_shape) == 3:  # Screen image is the input to the agent\n",
    "            if self.continuous_action_space:\n",
    "                self.actor= DeepActor(self.state_shape, self.action_shape, device).to(device)\n",
    "            else:  # Discrete action space\n",
    "                self.actor = DeepDiscreteActor(self.state_shape, self.action_shape, device).to(device)\n",
    "            self.critic = DeepCritic(self.state_shape, self.critic_shape, device).to(device)\n",
    "        else:  # Input is a (single dimensional) vector\n",
    "            if self.continuous_action_space:\n",
    "                #self.actor_critic = ShallowActorCritic(self.state_shape, self.action_shape, 1, self.params).to(device)\n",
    "                self.actor = ShallowActor(self.state_shape, self.action_shape, device).to(device)\n",
    "            else:  # Discrete action space\n",
    "                self.actor = ShallowDiscreteActor(self.state_shape, self.action_shape, device).to(device)\n",
    "            self.critic = ShallowCritic(self.state_shape, self.critic_shape, device).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.params[\"learning_rate\"])\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.params[\"learning_rate\"])\n",
    "\n",
    "        # Handle loading and saving of trained Agent models\n",
    "        episode_rewards = list()\n",
    "        prev_checkpoint_mean_ep_rew = self.best_mean_reward\n",
    "        num_improved_episodes_before_checkpoint = 0  # To keep track of the num of ep with higher perf to save model\n",
    "        #print(\"Using agent_params:\", self.params)\n",
    "        if self.params['load_trained_model']:\n",
    "            try:\n",
    "                self.load()\n",
    "                prev_checkpoint_mean_ep_rew = self.best_mean_reward\n",
    "            except FileNotFoundError:\n",
    "                if args.test:  # Test a saved model\n",
    "                    print(\"FATAL: No saved model found. Cannot test. Press any key to train from scratch\")\n",
    "                    input()\n",
    "                else:\n",
    "                    print(\"WARNING: No trained model found for this environment. Training from scratch.\")\n",
    "\n",
    "        for episode in range(self.params[\"max_num_episodes\"]):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            ep_reward = 0.0\n",
    "            step_num = 0\n",
    "            while not done:\n",
    "                action = self.get_action(obs)\n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                self.rewards.append(reward)\n",
    "                ep_reward += reward\n",
    "                step_num +=1\n",
    "                if not args.test and (step_num >= self.params[\"learning_step_thresh\"] or done):\n",
    "                    self.learn(next_obs, done)\n",
    "                    step_num = 0\n",
    "                    # Monitor performance and save Agent's state when perf improves\n",
    "                    if done:\n",
    "                        episode_rewards.append(ep_reward)\n",
    "                        if ep_reward > self.best_reward:\n",
    "                            self.best_reward = ep_reward\n",
    "                        if np.mean(episode_rewards) > prev_checkpoint_mean_ep_rew:\n",
    "                            num_improved_episodes_before_checkpoint += 1\n",
    "                        if num_improved_episodes_before_checkpoint >= self.params[\"save_freq_when_perf_improves\"]:\n",
    "                            prev_checkpoint_mean_ep_rew = np.mean(episode_rewards)\n",
    "                            self.best_mean_reward = np.mean(episode_rewards)\n",
    "                            self.save()\n",
    "                            num_improved_episodes_before_checkpoint = 0\n",
    "\n",
    "                obs = next_obs\n",
    "                self.global_step_num += 1\n",
    "                if args.render:\n",
    "                    self.env.render()\n",
    "                #print(self.actor_name + \":Episode#:\", episode, \"step#:\", step_num, \"\\t rew=\", reward, end=\"\\r\")\n",
    "                writer.add_scalar(self.actor_name + \"/reward\", reward, self.global_step_num)\n",
    "            print(\"{}:Episode#:{} \\t ep_reward:{} \\t mean_ep_rew:{}\\t best_ep_reward:{}\".format(\n",
    "                self.actor_name, episode, ep_reward, np.mean(episode_rewards), self.best_reward))\n",
    "            writer.add_scalar(self.actor_name + \"/ep_reward\", ep_reward, self.global_step_num)\n",
    "\n",
    "# The main execution block in the original script uses multiprocessing.\n",
    "# This is tricky to run directly in a notebook. \n",
    "# For a notebook, it's often better to run a single agent process for simplicity.\n",
    "if __name__ == \"__main__\":\n",
    "    agent_params = params_manager.get_agent_params()\n",
    "    agent_params[\"model_dir\"] = args.model_dir\n",
    "    agent_params[\"test\"] = args.test\n",
    "    env_params = params_manager.get_env_params()  # Used with Atari environments\n",
    "    env_params[\"env_name\"] = args.env\n",
    "    # mp.set_start_method('spawn') # This might be needed depending on the OS\n",
    "\n",
    "    print(\"Running a single agent process for notebook compatibility.\")\n",
    "    agent = DeepActorCriticAgent(0, args.env, agent_params, env_params)\n",
    "    agent.run() # Calling run() directly instead of start() and join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
